# ============================================================================
# Skin Analysis Service - ML Model Configuration
# ============================================================================
# This file contains all configuration options for the ML-powered skin
# analysis service. Copy this file to .env and adjust values as needed.
#
# Quick Start:
# - For GPU setup: Set DEVICE=auto and ENABLE_GPU=true
# - For CPU-only: Set DEVICE=cpu and ENABLE_GPU=false
# - For production: Enable caching and batch processing
# - For development: Enable debug mode and logging
# ============================================================================

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
# MongoDB connection string for storing analysis results
MONGODB_URL=mongodb://localhost:27017

# Database name for the skin analysis service
DATABASE_NAME=growup

# ============================================================================
# FILE UPLOAD SETTINGS
# ============================================================================
# Maximum file size for image uploads (in bytes)
# Default: 10MB (10485760 bytes)
MAX_FILE_SIZE=10485760

# Directory for storing uploaded and processed images
UPLOAD_DIR=./uploads

# ============================================================================
# LEGACY MODEL SETTINGS (Backward Compatibility)
# ============================================================================
# These settings are kept for backward compatibility with older versions
# Directory containing model files
MODELS_DIR=./models

# Path to legacy scikit-learn model (if used)
SKIN_MODEL_PATH=./models/skin_analysis_model.pkl

# Maximum time for legacy analysis (seconds)
MAX_ANALYSIS_TIME=5

# ============================================================================
# ML MODEL CONFIGURATION
# ============================================================================
# Model Selection
# Available models: efficientnet_b0 (recommended), resnet50, vit_base
# efficientnet_b0: Best balance of accuracy and speed (~20MB)
# resnet50: Good fallback option (~100MB)
# vit_base: Highest accuracy but slower (~330MB)
MODEL_NAME=efficientnet_b0

# Path to the trained model file (.pth format)
# This file is downloaded by setup scripts or can be trained locally
MODEL_PATH=./models/efficientnet_b0.pth

# Model version for tracking and compatibility
MODEL_VERSION=1.0.0

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================
# Device selection for model inference
# Options:
#   - auto: Automatically detect and use GPU if available, fallback to CPU
#   - cuda: Force GPU usage (will fail if GPU not available)
#   - cpu: Force CPU usage (slower but works everywhere)
# Recommended: auto (for flexibility)
DEVICE=auto

# Enable GPU acceleration if available
# Set to false to force CPU-only mode regardless of DEVICE setting
ENABLE_GPU=true

# GPU memory fraction to allocate (0.0 to 1.0)
# Lower values leave more memory for other processes
# Recommended: 0.8 for dedicated GPU, 0.5 for shared GPU
GPU_MEMORY_FRACTION=0.8

# ============================================================================
# INFERENCE PARAMETERS
# ============================================================================
# Minimum confidence threshold for reporting skin issues (0.0 to 1.0)
# Issues with confidence below this threshold are filtered out
# Higher values = fewer but more confident predictions
# Recommended: 0.7 for production, 0.5 for development
CONFIDENCE_THRESHOLD=0.7

# Batch size for inference
# Larger batches are faster but use more memory
# GPU: Can handle 4-8, CPU: Recommended 1-2
# Set to 1 for single image processing
BATCH_SIZE=1

# Maximum time allowed for inference (seconds)
# Inference will timeout if it exceeds this duration
MAX_INFERENCE_TIME=5

# ============================================================================
# MODEL ARCHITECTURE SETTINGS
# ============================================================================
# Number of skin type classes the model predicts
# Default: 5 (oily, dry, combination, sensitive, normal)
NUM_CLASSES_SKIN_TYPE=5

# Number of skin issue classes the model can detect
# Default: 8 (acne, dark_spots, wrinkles, redness, dryness, oiliness, enlarged_pores, uneven_tone)
NUM_CLASSES_ISSUES=8

# ============================================================================
# HUGGINGFACE CONFIGURATION
# ============================================================================
# Directory for caching downloaded models from HuggingFace
# Models are cached here to avoid re-downloading
HF_CACHE_DIR=./models/cache

# HuggingFace repository ID for downloading pre-trained models
# Used by setup scripts to download models
HF_REPO_ID=timm/efficientnet_b0.ra_in1k

# HuggingFace API token (optional)
# Required only for private models or to avoid rate limits
# Get your token from: https://huggingface.co/settings/tokens
# HF_TOKEN=your_huggingface_token_here

# ============================================================================
# MODEL LOADING SETTINGS
# ============================================================================
# Enable lazy loading (load model on first inference request)
# true: Faster startup, slower first request
# false: Slower startup, faster first request
# Recommended: true for development, false for production
LAZY_LOADING=true

# Preload model during service startup
# true: Model loaded immediately on startup
# false: Model loaded on first inference (if lazy loading enabled)
# Recommended: false for development, true for production
PRELOAD_MODEL=false

# Number of models to keep in memory cache
# Higher values use more memory but avoid reloading
# Recommended: 1 (most services use a single model)
MODEL_CACHE_SIZE=1

# ============================================================================
# PERFORMANCE OPTIMIZATION
# ============================================================================
# Enable model quantization for faster CPU inference
# Reduces model size and speeds up CPU inference by ~2-4x
# May slightly reduce accuracy (~1-2%)
# Recommended: false for GPU, true for CPU-only production
ENABLE_QUANTIZATION=false

# Enable ONNX runtime for optimized inference
# ONNX can provide 2-3x speedup on both CPU and GPU
# Requires ONNX model file (see ONNX_MODEL_PATH)
# Recommended: false (experimental feature)
ENABLE_ONNX=false

# Path to ONNX model file (required if ENABLE_ONNX=true)
# Generate ONNX model using: python scripts/export_to_onnx.py
# ONNX_MODEL_PATH=./models/efficientnet_b0.onnx

# ============================================================================
# PREDICTION CACHING
# ============================================================================
# Enable caching of prediction results
# Identical images will return cached results instantly
# Recommended: true for production (saves compute resources)
ENABLE_PREDICTION_CACHE=true

# Maximum number of predictions to cache (LRU cache)
# Higher values use more memory but increase cache hit rate
# Recommended: 128 for production, 32 for development
PREDICTION_CACHE_SIZE=128

# ============================================================================
# BATCH PROCESSING CONFIGURATION
# ============================================================================
# Enable batch processing for multiple images
# Processes multiple images in a single inference call
# Significantly faster for bulk operations
# Recommended: true
ENABLE_BATCH_PROCESSING=true

# Maximum batch size for batch processing
# Larger batches are faster but use more memory
# GPU: 8-16, CPU: 2-4
# Must be >= BATCH_SIZE
MAX_BATCH_SIZE=8

# ============================================================================
# MEMORY MANAGEMENT
# ============================================================================
# Automatically cleanup GPU/CPU memory after inference
# Helps prevent memory leaks and OOM errors
# Recommended: true
AUTO_CLEANUP_MEMORY=true

# Cleanup memory every N inferences
# Lower values = more frequent cleanup = less memory usage
# Higher values = less overhead = faster inference
# Recommended: 100
CLEANUP_INTERVAL=100

# ============================================================================
# PREPROCESSING SETTINGS
# ============================================================================
# Enable data augmentation during preprocessing
# Applies random transformations to improve robustness
# Only useful during training, not for inference
# Recommended: false for inference
ENABLE_AUGMENTATION=false

# Note: Normalization mean and std are hardcoded to ImageNet values
# NORMALIZE_MEAN=(0.485, 0.456, 0.406)
# NORMALIZE_STD=(0.229, 0.224, 0.225)

# ============================================================================
# POST-PROCESSING SETTINGS
# ============================================================================
# Minimum confidence threshold for any prediction (0.0 to 1.0)
# Predictions below this are completely discarded
# Should be lower than CONFIDENCE_THRESHOLD
MIN_CONFIDENCE_THRESHOLD=0.5

# Maximum confidence threshold (0.0 to 1.0)
# Predictions above this are considered highly confident
# Used for quality filtering and reporting
MAX_CONFIDENCE_THRESHOLD=0.95

# Enable generation of highlighted images showing detected areas
# Creates visual overlays highlighting skin issues
# Recommended: true (provides valuable user feedback)
ENABLE_HIGHLIGHTED_IMAGES=true

# Color for highlighting detected areas
# Options: red, green, blue, yellow, orange, purple
# Recommended: red (most visible)
HIGHLIGHT_COLOR=red

# Transparency of highlight overlay (0.0 to 1.0)
# 0.0 = fully transparent, 1.0 = fully opaque
# Recommended: 0.3 (visible but not overwhelming)
HIGHLIGHT_ALPHA=0.3

# ============================================================================
# FALLBACK API CONFIGURATION (Optional)
# ============================================================================
# Enable fallback to external API when local model fails or has low confidence
# Useful for ensuring service availability and handling edge cases
# Recommended: false for development, true for production (if available)
ENABLE_FALLBACK_API=false

# URL of the fallback API endpoint
# Required if ENABLE_FALLBACK_API=true
# Example: https://api.example.com/v1/skin-analysis
# FALLBACK_API_URL=https://api.example.com/skin-analysis

# API key for authenticating with fallback API
# Required if fallback API requires authentication
# FALLBACK_API_KEY=your_api_key_here

# Timeout for fallback API requests (seconds)
# Prevents hanging on slow external APIs
FALLBACK_API_TIMEOUT=10

# Use fallback API when local inference fails with an error
# Recommended: true (ensures service availability)
FALLBACK_ON_ERROR=true

# Use fallback API when local model confidence is below threshold
# Recommended: false (may increase costs)
FALLBACK_ON_LOW_CONFIDENCE=false

# ============================================================================
# LOGGING AND MONITORING
# ============================================================================
# Log inference time for each prediction
# Useful for performance monitoring and optimization
# Recommended: true
LOG_INFERENCE_TIME=true

# Log memory usage during inference
# Useful for debugging memory issues
# May add slight overhead
# Recommended: false for production, true for debugging
LOG_MEMORY_USAGE=false

# Enable metrics collection (Prometheus-compatible)
# Collects inference time, throughput, error rates, etc.
# Recommended: true for production
ENABLE_METRICS=true

# ============================================================================
# ERROR HANDLING
# ============================================================================
# Retry inference on transient errors
# Helps handle temporary GPU/memory issues
# Recommended: true
RETRY_ON_ERROR=true

# Maximum number of retry attempts
# Higher values increase resilience but may delay error reporting
# Recommended: 3
MAX_RETRIES=3

# Delay between retry attempts (seconds)
# Gives time for transient issues to resolve
# Recommended: 1.0
RETRY_DELAY=1.0

# ============================================================================
# DEVELOPMENT AND DEBUG SETTINGS
# ============================================================================
# Enable debug mode for verbose logging
# Logs detailed information about preprocessing, inference, and post-processing
# Recommended: false for production, true for development
DEBUG_MODE=false

# Save preprocessed images to disk for inspection
# Useful for debugging preprocessing pipeline
# Images saved to UPLOAD_DIR with 'preprocessed_' prefix
# Recommended: false (uses disk space)
SAVE_PREPROCESSED_IMAGES=false

# Save attention maps showing model focus areas
# Useful for understanding model decisions
# Images saved to UPLOAD_DIR with 'attention_' prefix
# Recommended: false (uses disk space and compute)
SAVE_ATTENTION_MAPS=false

# ============================================================================
# SERVICE CONFIGURATION
# ============================================================================
# Port for the FastAPI service
PORT=8001

# Environment (development, staging, production)
ENVIRONMENT=development

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# DEBUG: Very verbose, all details
# INFO: General information, recommended for development
# WARNING: Only warnings and errors
# ERROR: Only errors
# Recommended: INFO for development, WARNING for production
LOG_LEVEL=INFO

# ============================================================================
# CONFIGURATION EXAMPLES
# ============================================================================
#
# Example 1: GPU Development Setup (Recommended for local development with GPU)
# ---------------------------------------------------------------------------
# DEVICE=auto
# ENABLE_GPU=true
# GPU_MEMORY_FRACTION=0.8
# CONFIDENCE_THRESHOLD=0.7
# BATCH_SIZE=1
# LAZY_LOADING=true
# PRELOAD_MODEL=false
# ENABLE_PREDICTION_CACHE=true
# PREDICTION_CACHE_SIZE=32
# DEBUG_MODE=true
# LOG_LEVEL=INFO
#
# Example 2: CPU-Only Development Setup (For machines without GPU)
# ---------------------------------------------------------------------------
# DEVICE=cpu
# ENABLE_GPU=false
# CONFIDENCE_THRESHOLD=0.7
# BATCH_SIZE=1
# LAZY_LOADING=true
# PRELOAD_MODEL=false
# ENABLE_QUANTIZATION=true  # Speed up CPU inference
# ENABLE_PREDICTION_CACHE=true
# PREDICTION_CACHE_SIZE=32
# DEBUG_MODE=true
# LOG_LEVEL=INFO
#
# Example 3: Production GPU Setup (High performance, high availability)
# ---------------------------------------------------------------------------
# DEVICE=auto
# ENABLE_GPU=true
# GPU_MEMORY_FRACTION=0.9
# CONFIDENCE_THRESHOLD=0.75
# BATCH_SIZE=4
# LAZY_LOADING=false
# PRELOAD_MODEL=true
# ENABLE_PREDICTION_CACHE=true
# PREDICTION_CACHE_SIZE=256
# ENABLE_BATCH_PROCESSING=true
# MAX_BATCH_SIZE=16
# ENABLE_FALLBACK_API=true
# FALLBACK_API_URL=https://api.example.com/skin-analysis
# FALLBACK_ON_ERROR=true
# ENABLE_METRICS=true
# DEBUG_MODE=false
# LOG_LEVEL=WARNING
#
# Example 4: Production CPU Setup (Cost-effective, no GPU required)
# ---------------------------------------------------------------------------
# DEVICE=cpu
# ENABLE_GPU=false
# CONFIDENCE_THRESHOLD=0.75
# BATCH_SIZE=2
# LAZY_LOADING=false
# PRELOAD_MODEL=true
# ENABLE_QUANTIZATION=true  # Essential for CPU performance
# ENABLE_PREDICTION_CACHE=true
# PREDICTION_CACHE_SIZE=256
# ENABLE_BATCH_PROCESSING=true
# MAX_BATCH_SIZE=4
# ENABLE_FALLBACK_API=true
# FALLBACK_API_URL=https://api.example.com/skin-analysis
# FALLBACK_ON_ERROR=true
# ENABLE_METRICS=true
# DEBUG_MODE=false
# LOG_LEVEL=WARNING
#
# Example 5: Testing/CI Setup (Fast, minimal resources)
# ---------------------------------------------------------------------------
# DEVICE=cpu
# ENABLE_GPU=false
# CONFIDENCE_THRESHOLD=0.5
# BATCH_SIZE=1
# LAZY_LOADING=true
# PRELOAD_MODEL=false
# ENABLE_PREDICTION_CACHE=false
# ENABLE_QUANTIZATION=true
# DEBUG_MODE=true
# LOG_LEVEL=DEBUG
#
# ============================================================================
